#+TITLE: Adding a Serverless Commenting System to My Blog
Mon 01/14/19
--------------------------------------------------------------------------------

Serverless web technologies are awesome. No longer do we need to pay for servers to spin 24/hours a day to listen for a request
that may never come. Instead we can pay per request, and with a little JavaScript, make the users computer do the bulk of the work! Excellent.

My goal for this post is explain how I implemented a serverless commenting system with AWS. As I was doing this, there were two sides of the
user experience to think about: the blog readers, who will be reading and submitting comments, and myself, who will be creating the posts. From
the users point of view, the requirements are simple:

+ an input for their name
+ a text area for the comment body
+ a button to submit
+ an area to show previously submitted comments, newest first

To accomplish this I wrote a footer to be included in every post:

footer.html
#+BEGIN_SRC html
<div id="footer">
  <h3>Comments</h3>
  <form>
    <input id="name" placeholder="your name" maxlength=16 required>
    <textarea id="comment-body" placeholder="your comment" required></textarea>
    <button id="submit-button" type="button" onclick="saveComment()"> Submit </button>
  </form>
  <div id="comments">
    There's nothing here yet. Be the first!
  </div>
</div>
#+END_SRC

Next I had to think about my user perspective as the post publisher; would I need to do anything
extra to enable commenting on a new post? The first problem I had was how to include the footer into every post 
that I write. Furthermore, how will that post know which number it is so that it can save comments that
are associated with itself? If I were using a web server with php it would be trivial:

#+BEGIN_SRC php
<?php
  $post_number = 6;
  include 'partial/footer.php';
?>
#+END_SRC

It was crucial that my workflow for publishing a post be no different than it has been:

1. `$ vim 6.md` - write the post in a markdown
2. `$ git add 6.md && git commit -m "write post 6" && git push` - so that I can pull and edit the post on other machines
3. `$ create-post 6` - compile `6.md` into `6.html` and update the table in `blog.html`
4. `$ upload-site` - upload `6.html` to my [S3](5.html) bucket
5. and voila! this post is available on the web

Adding `footer.html` to the end of all my posts was straightforward as `create-post` already included header.html
, which is the partial file that creates the grey bar at the top of this page. The only minor hiccup was 
finding a way to tell the footer which post number it was. I initially thought to edit the footer after reading 
it, and change the `onclick="saveComment()"` to have the post number as a parameter, something like 
`onclick="saveComment(6)"`. That would either lead to messy python or regex sorcery. Either way it wouldn't be 
very easy to read or maintain. A more simple method is to simply include a script tag with the post number 
declared as a constant above the footer markup.

new code in [create-post](https://github.com/knoebber/personal-website/blob/master/scripts/create-post)

#+BEGIN_SRC python
# read header and footer
h = open(partial_dir+'/header.html')
header = h.read()
h.close()
f = open(partial_dir+'/footer.html')
footer = f.read()
f.close()
 # add a post number variable to the footer so it knows how to save comments
footer = "<script> const postNum = "+post_num + ";</script>" + footer;
 # create post html from header, markdown, and footer
html = markdown(md.read())
html = header + '\n' + html + '\n' + footer
md.close()
#+END_SRC

With the markup done, there still remained the tasks of creating a back end to process/save the comments,
and writing the client side JavaScript to interact with this. I chose to write the back end first, 
with the following tasks in mind:

1. create database schema
2. create function to write to database
3. create function to read from database

Normally I would add delete and edit as well, but in this case I stuck to only to read and write.
To delete and edit there needs to be a way to tell which user owns the content, and I do not plan on adding
a user login system to this project. Not only would that be way outside the scope of what I wanted to accomplish,
but it would take away from how I want it to work - a simple system to leave and read comments that doesn't
get in your way.

*# Creating a Table in DynamoDB

DynamoDB is a NoSQL database that can scale to any level. You can either choose to pay for the amount
read/writes that you want to allow per second, or you can let it auto scale, up to a claimed 20 million
requests per second. I went with the former, which is called provisioned capacity.

[[file:../../images/dynamo_scaling.png]]

So my table is able to handle 2 requests per second for now, but it is as simple as changing the values in
the "Provisioned Capacity" box if this proves to be too little. One of things that I struggled with when
setting up my comments table is the "NoSQL" bit. I didn't quite grasp that it means exactly what it says:
DynamoDB is +not+ SQL. Where a SQL table has a set amount of columns and a robust language to query based on
any of these columns, a DynamoDB table has one primary key column which uniquely identifies any kind of JSON
object that is stored in the table.
The great part of NoSQL is that it allows for any structure of data to be stored in it, the bad part is that it
makes efficient SQL like queries for non primary key attributes impossible.

As I've worked with SQL a fair amount, my first instinct was to design a table that looked something like this.

--------------------------------------------------------------------------------
~id (primary key), post_number, name, comment, time_stamp++~
--------------------------------------------------------------------------------

The problem with this is that the comments must be pulled by their post number, and ordered by their time stamp.
After I created this table in DynamoDB, I quickly realized that I would have to iterate through every row in 
the table, picking only the comments with the post number that I wanted.
Then I would have to sort the resulting list by `time_stamp`. A far cry from a simple SQL query like this:

#+BEGIN_SRC sql
SELECT time_stamp, name, body FROM comments WHERE post_number = 6 ORDER BY time_stamp;
#+END_SRC

I needed a primary key that would always be unique, which I could also use to get all the comments from a
specific post. At this point I dove into the DynamoDB docs, and found that it is possible to create a
composite primary key. A composite primary key is combination of a partition key and sort key, where
multiple items may share the same partition key, but their sort keys must be unique. I dropped the `id` column,
made a partition key for `post_number`, and a sort key for `time_stamp`.

[[file:../../images/comments_table.png]]

Note that the `comment_body` and `comment_name` columns were not specified at all in the creation of this table.
I can actually submit data with +any+ key name to this table, and it would create a new column for it. The only
requirements are that `post_number` exists and that `time_stamp` is unique. This also solves the problem of how
to sort the resulting data - the sort key does this automatically when the table is queried.

*# Making API calls to Lambda for DynamoDB operations

With the `comment` table setup, the next task was to create some back end functions to perform read/write 
operations. I chose to use Node.js with Lamdba to accomplish this. Lambda is an AWS service that lets you upload 
code which will be ran depending on how you configure its triggers. The main benefit of Lambda is that you pay 
for the execution time of the code rather than paying for a server to run 24/7. I set up my Lambda functions to 
use a trigger from API Gateway, an AWS service that lets you set up RESTful API routes.

[[file:../../images/lambda-triggers.png]]

Writing the Node.js to communicate with DynamoDB was straightforward, and before long I had two routes that I
could call from cURL that saved and retrieved comments: <br>
[get comments handler](https://github.com/knoebber/personal-website/blob/master/lambda/get_comments/index.js)<br>
[post comment handler](https://github.com/knoebber/personal-website/blob/master/lambda/post_comment/index.js)

Great, I thought. Now all I had to was add some JavaScript to the
footer to handle the submit button, and I was done. I started with something like this:

addition to [footer.html](https://github.com/knoebber/personal-website/blob/master/posts/partial/footer.html)

#+BEGIN_SRC javascript
 function saveComment(){
  fetch('https://l4oejeyzok.execute-api.us-west-2.amazonaws.com/default/post_comment', {
    method: 'POST',
    body: JSON.stringify({
      postNumber:postNum,
      commentName:name,
      commentBody:comment
    }),
    headers: {
      'Content-Type':'application/json'
    }
  }).then(response => response.json())
    .then(data     => displayComment(data))
    .catch(err     => console.log(err));
}
 function displayComment(comment){
  const comments = document.getElementById('comments');
   // Remove the "There's nothing here yet" text on the first comment insert.
  if (comments.children.length == 0 ) {
    comments.innerHTML = ''
  }
  const date = new Date(parseInt(comment.time_stamp.N))
  const year = date.getFullYear();
  const month = date.getMonth() + 1;
  const day = date.getDate();
  let newComment = document.createElement('div');
  newComment.classList.add("comment");
  newComment.innerHTML = `
    <div class="comment-name">
       <strong>${comment.comment_name.S}<span class="date">${month}/${day}/${year}</span></strong>
    </div>
    <div class="comment-body">
      ${comment.comment_body.S}
    </div>`;
   comments.prepend(newComment);
}
#+END_SRC
I
So I eagerly hit the submit comment button ... 
only to see a yellow message pop up in my console: "Cross-Origin Request Blocked". After some googling, I learned
that my browser was protecting me from fetching resources from a different server than the page was hosted on. On
a classic web server setup, the html files are hosted on the same server that the back end is on, so browsers
added this as a security check to protect against malicious scripts fetching resources from foreign servers.
At this point I hit the biggest roadblock that I faced in the project - figuring out how to get around this.

I learned that cross origin requests can be allowed through CORS, or Cross Origin Resource
Sharing. This can be enabled on the server that the resources are being requested from. At first I thought
all I had to do was add a header to the HTTP response from Lambda:

#+BEGIN_SRC javascript
const respond = (code,response) => callback(null,
  {
    statusCode:code,
    body: JSON.stringify(response),
    headers:{
      "Access-Control-Allow-Origin" : "+"
    }
  })
#+END_SRC
The important bit here is `"Access-Control-Allow-Origin":"+"`, which means "allow any website to request
this resource". I could change the `+` to `nicolasknoebber.com`, but I test this often from localhost, so I chose
to leave it as the wild card. So with my new shiny Node.js code uploaded to Lambda, I hit the submit comment
button again, only to see the same notice show up. My first instinct was to test that the header was actually
being sent in the response, so I hit the end point with cURL's verbose option. It was indeed being sent back. 

I went back to the AWS docs, and eventually found [this article](https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html). 
Basically, in addition to the `Access-Control-Allow-Origin` header, I would need to create another method
in API Gateway, a so called "Preflight" check. Luckily, API Gateway automates this process.

[[file:../../images/api-gateway-cors.png]]

So when a script in one of my blog posts makes an API call to AWS, it will first send an OPTIONS request,
which API Gateway will respond back and say OK, this CORS request can go through. After receiving this reply,
the actual POST request will be sent out that saves the comment.

*# Finishing up

The rest of the project came together quickly once I was able to contact Lambda from JavaScript embedded in the 
footer. I made another `fetch` call to get all the comments for the post using the `/get_comments` API route 
with the `postNum` constant as its parameter. Next I added some CSS to the comments so that they float in
rather than appearing suddenly. I also added some preventative measures to protect
against the user spamming the API. While I could of used Captcha, I would rather say no to having my
readers train Googles AI. Instead I went with preventing the same comment from being submitted twice by
creating a set of all the current comments on the page , and checking if the new comment exists on submission. On
top of this I disabled the submit button while a create request is still asynchronously processing (because
I know how we love to keep clicking a button when it doesn't work instantly).

More importantly I have auto scaling set to off in my AWS services, so if my API does get flooded, I won't have
to pay for it. I would like to build an email notification system so I know when a new comment is made, 
which would let me moderate more easily.
