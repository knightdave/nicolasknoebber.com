## Moving My Website to Amazon S3
#### Tue 07/24/18

---

For the last year I've had this website hosted on an Amazon EC2 instance, which has cost about $10 a month.
That's not too expensive,  but with S3 the cost can be almost nothing, depending on how much traffic I get.

S3, or [Amazon's simple storage service](https://aws.amazon.com/s3 "s3"), is a bit like Google Drive, only much more powerful. 
Basically S3 lets you create "buckets", which are unique storage devices kept in the cloud. S3 come with a powerful API so that uploads and downloads can be automated through scripting, and most importantly, 
options that allow their contents to be shared with the world wide web.

The first step to hosting on S3 is to create a new bucket. The name must be unique across all of AWS.
This should match up with the websites domain name, in my case, *nicolasknoebber.com* .

![my new bucket](images/new_bucket.png)

I left all the settings as their defaults. After its created, the websites file can be uploaded into it. This can
be done manually with their web GUI, or through their API. I personally prefer the latter, and I recommend the command line tool `awscli` for managing the calls.
`awscli` can be installed through most system package managers or `pip`. Before using it, an access key must be obtained from the IAM section of AWS.
With an access key handy run `aws configure` and follow the instructions. Once its setup, the vast power of Amazon Web Services is ready to be wrangled from the comfort of a terminal. 
The possibilities are endless!

The tool is invoked with `aws` followed by the service that is being used, and then a function for that service. When in doubt, use `aws help`. In this case I want to create an alias for 
syncing what I have locally with my bucket. So if I have a file that's not on the bucket, that file will be uploaded. Or if the bucket has something I don't, it will download that for me. This will be
extremely convenient for deploying changes to my site. I'll add the following to my `.bash_profile`.

    alias sync-nk='aws s3 sync ~/code/personal-website s3://nicolasknoebber.com --exclude="*.swp" --exclude=".git/*"'

Note the exclude flags at the end - these ignore files from the sync with wild cards. I don't want my git tree to be uploaded, and I don't want .swp files from
buffers I have open in vim. Now I can call `sync-nk` in my websites folder on my local machine, and all the files will be uploaded. Neat!

Now for the important part: making these uploaded html files public on the web.
This is surprisingly simple. In a web browser, navigate to the bucket in the AWS web app. 
Click the properties tab and then "Static website hosting". Enter the name of the index document, and then save it.

![static hosting](images/static_hosting.png)

The bucket is now hosted on the web, and can be viewed by clicking on the link.

However, this isn't public. To make it so that anyone can read it, head over to the permissions tab
and then bucket policy. I added this to the text area, which allows anyone to view the bucket:

		{
				"Version": "2012-10-17",
				"Statement": [
						{
								"Sid": "PublicReadGetObject",
								"Effect": "Allow",
								"Principal": "*",
								"Action": "s3:GetObject",
								"Resource": "arn:aws:s3:::nicolasknoebber.com/*"
						}
				]
		}

We aren't done yet though. I could tell all my friends to visit *http://nicolasknoebber.com.s3-website-us-west-2.amazonaws.com/* but I would rather have it just be *nicolasknoebber.com*.
This is a job for DNS, or Domain Name System. This allows for websites to be given shorter names, instead of having everyone remember long unwieldy addresses.
To accomplish this, I'll have to create another bucket for *www.nicolasknoebber.com*. Create it exactly the same way, but this time under the static web hosting menu,
click 'Redirect all requests', and then the original bucket name. This is necessary for the upcoming DNS configuration.

There are many DNS services, but to keep with the Amazon theme, I'll use [Amazon Route 53](https://aws.amazon.com/route53/) 
From here navigate into hosted zones. I'll create a new hosted zone with the same name as my bucket, *nicolasknoebber.com*

This generate a NS (name server) type row. Now Route53 needs to be told to point the traffic
towards S3. Create a record set named `www`. For the alias target I put this:

    s3-website-us-west-2.amazonaws.com

![record set](images/hosted_zone.png)

From here create another record set, the same as before but with the name field empty. This makes it
so that 'www' won't need to be entered before the url when navigating to this website.

The final step is to configure your domain name. Though AWS has a service for this as well, I have mine registered on Google Domains. So at least one part of my stack won't be Amazon!
Open up the DNS settings on the domain and click 'Use custom name servers'. Copy the 4 entries from the Route53 NS row and add them here one by one.

![google domains](images/google_domains.png)

And that's it! Or so I thought. Turns out DNS settings take a while to come into affect. I had to wait about 10 hours before I could hit my website from `nicolasknoebber.com`.
