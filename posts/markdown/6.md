## Adding a Serverless Commenting System to My Blog
#### Sat 01/12/19

---

Serverless web technologies are awesome. No longer do we need to pay for servers to spin 24/hours a day to listen for a request
that may never come. Instead we can pay per request, and with a little JavaScript, make the users computer do the bulk of the work! Excellent.

My goal for this post is explain how I implemented a serverless commenting system with AWS. As I was doing this, there were two sides of the
user experience to think about: the blog readers, who will be reading and submitting comments, and myself, who will be creating the posts. From
the users point of view, the requirements are simple:

* an input for their name
* a text area for the comment body
* a button to submit
* an area to show previously submitted comments, newest first

To accomplish this I added a footer to every blog post:

#### footer.html

    <div id="footer">
      <h3>Comments</h3>
      <form>
        <input id="name" placeholder="your name" maxlength=16 required>
        <textarea id="comment-body" placeholder="your comment" required></textarea>
        <button id="submit-button" type="button" onclick="saveComment()"> Submit </button>
      </form>
      <div id="comments">
        There's nothing here yet. Be the first!
      </div>
    </div>

With the footer written, next I had to think about my user perspective as the post publisher.
The first problem I had was how I am going to include this into every post that I write.
Furthermore, how will that post know which number it is so that it can save comments that
are associated with itself? If I were using a web server with php it would be trivial:

    <?php
      $post_number = 6;
      include 'partial/footer.php';
    ?>

It was crucial that my workflow for publishing a post be no different than it has been:

1. `$ vim 6.md` - write the post in a markdown
2. `$ git add 6.md && git commit -m "write post 6" && git push` - so that I can pull and edit the post on other machines
3. `$ create-post 6` - compile `6.md` into `6.html` and update the table in `blog.html`
4. `$ upload-site` - upload `6.html` to my S3 bucket
5. and voila! this post is available on the web

Including `footer.html` into all my posts turned out to be straightforward as my program for post creation
already had the functionality of including the header (the grey bar with 'blog' at the top this page).
So I needed to open `footer.html` and append its contents to the new html file.  The only minor hiccup was
finding a way to tell the footer which post number it was.
I initially thought to edit the footer after reading it, and change the `onclick="saveComment()"` to have
the post number as a parameter, something like `onclick="saveComment(6)"`.
That would either lead to messy python or regex sorcery. Either way it wouldn't be very easy to read or maintain.
A more simple method is to simply include a script tag with the post number declared as a constant above the
footer markup.

#### new code in <a href="https://github.com/knoebber/personal-website/blob/master/scripts/create-post">create-post</a>

    # read header and footer
    h = open(partial_dir+'/header.html')
    header = h.read()
    h.close()
    f = open(partial_dir+'/footer.html')
    footer = f.read()
    f.close()

    # add a post number variable to the footer so it knows how to save comments
    footer = "<script> const postNum = "+post_num + ";</script>" + footer;

    # create post html from header, markdown, and footer
    html = markdown(md.read())
    html = header + '\n' + html + '\n' + footer
    md.close()

With the markup done, there still remained the tasks of creating a back end to process/save the comments,
and writing the client side JavaScript to interact with this.
I chose to write the back end first, going in the following order:

1. create database schema
2. create function to write to database
3. create function to read from database

Normally I would add delete and edit as well, but in this case I stuck to only to read and write.
To delete and edit there needs to be a way to tell which user owns the content, and I do not plan on adding
a user login system to this project. Not only would that be way outside the scope of what I wanted to accomplish,
but it would take away from how I want it to work - a simple system to leave and read comments that doesn't
get in your way.

I used the following AWS services to create this back end:

* DynamoDB
* Lambda
* API Gateway

### Creating a Table in DynamoDB

DynamoDB is a NoSQL database that can scale to any level. You can either choose to pay for the amount
read/writes that you want to allow per second, or you can let it auto scale, up to a claimed 20 million
requests per second. I went with the former, which is called provisioned capacity.

![dynamo_scaling](images/dynamo_scaling.png)

So my table is able to handle 2 requests per second for now, but it is as simple as changing the values in
the "Provisioned Capacity" box if this proves to be too little. One of things that I struggled with when
setting up my comments table is the "NoSQL" bit. I didn't quite grasp that it means exactly what it says:
DynamoDB is *not* SQL. Where a SQL table has a set amount of columns and a robust language to query based on
any of these columns, a DynamoDB table has one primary key column which uniquely identifies any kind of JSON
object that is stored in the table.
The good part NoSQL is that it allows for any structure of data to be stored in it, the bad part is that it
makes efficient SQL like queries for non primary key attributes impossible.

As I've worked with SQL a fair amount, my first instinct was to design a table that looked something like this.

---
**id (primary key), post_number, name, comment, time_stamp**

---

The problem with this is that the comments must be pulled by their post number, and ordered by their time stamp.
After I created this table in DynamoDB, I quickly realized that I would have to scan every row in the table
every time, and then pick only the comments with the post number that I wanted.
Then I would have to sort the resulting list by `time_stamp`. A far cry from a simple SQL query like this:

    SELECT time_stamp, name, body FROM comments WHERE post_number=$post_number ORDER BY time_stamp

I needed a primary key that would always be unique, which I could also use to get all the comments from a
specific post. At this point I dove into the DynamoDB docs, and found that it is possible to create a
composite primary key. A composite primary key is combination of a partition key and sort key, where
multiple items may share the same partition key, but their sort keys must be unique. I dropped the `id` column,
made a partition key for `post_number`, and a sort key for `time_stamp`.

![comments_table](images/comments_table.png)

Note that the `comment_body` and `comment_name` columns were not specified at all in the creation of this table.
I can actually submit data with *any* key name to this table, and it would create a new column for it. The only
requirements are that `post_number` exists and that `time_stamp` is unique. This also solves the problem of how
to sort the resulting data - the sort key does this automatically when the table is queried.

### Making API calls to Lambda for DynamoDB operations

With the `comment` table setup, the next task was to create some back end functions to perform read/write operations on
the it. I chose to use Node.js with Lamdba to accomplish this. Lambda is an AWS service that lets you upload code which
will be ran depending on how you configure its triggers.  It's nice because you pay for execution time of the code instead of
paying for a server to run 24/7.
